{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c3c4d72",
   "metadata": {},
   "source": [
    "# LLM Project Notebook: Topic Classification on 20 Newsgroups\n",
    "This notebook implements a complete NLP pipeline including data loading, preprocessing, vectorization, baseline modeling, fine-tuning a transformer model, and pushing it to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522b14be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate\n",
    "from huggingface_hub import notebook_login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf7f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fair number of brave souls who upgraded thei...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nDo you have Weitek's address/phone number?  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  I was wondering if anyone out there could enli...      7   \n",
       "1  A fair number of brave souls who upgraded thei...      4   \n",
       "2  well folks, my mac plus finally gave up the gh...      4   \n",
       "3  \\nDo you have Weitek's address/phone number?  ...      1   \n",
       "4  From article <C5owCB.n3p@world.std.com>, by to...     14   \n",
       "\n",
       "              label_text  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‚úÖ Step 2: Load Dataset\n",
    "dataset = load_dataset(\"SetFit/20_newsgroups\")\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29259a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/snoopy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/snoopy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/snoopy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>wondering anyone could enlighten car saw day d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fair number of brave souls who upgraded thei...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>fair number brave soul upgraded clock oscillat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>well folk mac plus finally gave ghost weekend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nDo you have Weitek's address/phone number?  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>weitek address phone number like get informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>article tombaker world std com tom baker under...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  I was wondering if anyone out there could enli...      7   \n",
       "1  A fair number of brave souls who upgraded thei...      4   \n",
       "2  well folks, my mac plus finally gave up the gh...      4   \n",
       "3  \\nDo you have Weitek's address/phone number?  ...      1   \n",
       "4  From article <C5owCB.n3p@world.std.com>, by to...     14   \n",
       "\n",
       "              label_text                                         clean_text  \n",
       "0              rec.autos  wondering anyone could enlighten car saw day d...  \n",
       "1  comp.sys.mac.hardware  fair number brave soul upgraded clock oscillat...  \n",
       "2  comp.sys.mac.hardware  well folk mac plus finally gave ghost weekend ...  \n",
       "3          comp.graphics  weitek address phone number like get informati...  \n",
       "4              sci.space  article tombaker world std com tom baker under...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‚úÖ Step 3: Preprocessing\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)  # remove HTML\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # keep only letters\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04727c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 4: Vectorization (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "y = df[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccea087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7344233318603623\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.66      0.64        97\n",
      "           1       0.66      0.73      0.69       104\n",
      "           2       0.71      0.70      0.70       115\n",
      "           3       0.67      0.64      0.66       123\n",
      "           4       0.83      0.63      0.71       126\n",
      "           5       0.76      0.86      0.81       106\n",
      "           6       0.65      0.73      0.69       109\n",
      "           7       0.80      0.74      0.77       139\n",
      "           8       0.73      0.75      0.74       122\n",
      "           9       0.52      0.85      0.65       102\n",
      "          10       0.90      0.81      0.85       108\n",
      "          11       0.90      0.85      0.87       125\n",
      "          12       0.68      0.68      0.68       114\n",
      "          13       0.81      0.82      0.81       119\n",
      "          14       0.80      0.82      0.81       127\n",
      "          15       0.73      0.78      0.75       122\n",
      "          16       0.78      0.76      0.77       121\n",
      "          17       0.85      0.76      0.80       102\n",
      "          18       0.73      0.73      0.73       107\n",
      "          19       0.57      0.21      0.31        75\n",
      "\n",
      "    accuracy                           0.73      2263\n",
      "   macro avg       0.73      0.73      0.72      2263\n",
      "weighted avg       0.74      0.73      0.73      2263\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tfidf_logreg_model.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‚úÖ Step 5: Train Baseline Model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(clf, \"tfidf_logreg_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "417dc393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      "Oh, Your Highness?   And exactly why \"should\" the quotation\n",
      "marks enclose \"laws,\" not \"must.\"\n",
      "\n",
      "In c\n",
      "Prediction: joy (Confidence: 0.3811)\n",
      "\n",
      "Text: Hi again!\n",
      "\n",
      "    Okay, am getting an old AT type together as well.\n",
      "Anyone have a 16 bit MFM HDC they'd\n",
      "Prediction: joy (Confidence: 0.9962)\n",
      "\n",
      "Text: \n",
      "----------\n",
      "Prediction: anger (Confidence: 0.6872)\n",
      "\n",
      "Text: #>In article <1993Apr15.222600.11690@research.nj.nec.com>  \n",
      "#>>  ...\n",
      "#>> \tSeveral chemists already h\n",
      "Prediction: anger (Confidence: 0.6898)\n",
      "\n",
      "Text: Oh, excuse me for wasting the bandwidth, but I was referring to \n",
      "the original incident, not the rece\n",
      "Prediction: anger (Confidence: 0.9893)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 6: Try a Pretrained Inference Pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\")\n",
    "sample_texts = df[\"text\"].sample(5).tolist()\n",
    "for text in sample_texts:\n",
    "    result = pipe(text[:512])[0]\n",
    "    print(f\"Text: {text[:100]}\")\n",
    "    print(f\"Prediction: {result['label']} (Confidence: {result['score']:.4f})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0f42fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 7: Hugging Face Login (Secure)\n",
    "#notebook_login()\n",
    "from huggingface_hub import login\n",
    "login(token=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff898d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3ac4f4e67146f4af7440f3194ccf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7532 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/transformers/training_args.py:1583: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/var/folders/x6/k6smyqdj3lg6qmw8y7wds1jc0000gn/T/ipykernel_25541/388463901.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d45022bc1d4bcb9f841d142344ea19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.001, 'grad_norm': 2.634221076965332, 'learning_rate': 4.126984126984127e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04148375ef54dbabb45d4aea0ce3760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0000641345977783, 'eval_accuracy': 0.075, 'eval_runtime': 3.8055, 'eval_samples_per_second': 52.555, 'eval_steps_per_second': 3.416, 'epoch': 1.0}\n",
      "{'train_runtime': 71.3338, 'train_samples_per_second': 14.019, 'train_steps_per_second': 0.883, 'train_loss': 3.0031339251805864, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./finetuned_model/tokenizer_config.json',\n",
       " './finetuned_model/special_tokens_map.json',\n",
       " './finetuned_model/vocab.txt',\n",
       " './finetuned_model/added_tokens.json',\n",
       " './finetuned_model/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‚úÖ Step 8: Fine-Tune Transformer Model\n",
    "# Disable W&B and Tokenizer Parallelism\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# Force CPU (avoid MPS backend)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load Tiny Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\",\n",
    "    num_labels=20   \n",
    ").to(device)        # Move model to CPU\n",
    "\n",
    "# Tokenization Function \n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512   \n",
    "    )\n",
    "\n",
    "# Prepare Dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Subset Dataset for Fast Training\n",
    "train_ds = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(1000))  # Smaller training set\n",
    "eval_ds = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(200))     # Smaller eval set\n",
    "\n",
    "# Define Evaluation Metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,        # Safe for small model\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,\n",
    "    push_to_hub=False,                     \n",
    "    evaluation_strategy=\"epoch\",           # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                 # Save model at the end of each epoch\n",
    "    load_best_model_at_end=True,\n",
    "    no_cuda=True                           # Force CPU (disable GPU/MPS)\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "trainer.train()\n",
    "\n",
    "# Save Model and Tokenizer Locally\n",
    "model.save_pretrained(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce2d85a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/snoopy/LHL-Project-5-LLM/wandb/run-20250601_162703-r3buk1f8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alinatendler-self-employed/LLM-LHL/runs/r3buk1f8' target=\"_blank\">dutiful-snowball-1</a></strong> to <a href='https://wandb.ai/alinatendler-self-employed/LLM-LHL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alinatendler-self-employed/LLM-LHL' target=\"_blank\">https://wandb.ai/alinatendler-self-employed/LLM-LHL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alinatendler-self-employed/LLM-LHL/runs/r3buk1f8' target=\"_blank\">https://wandb.ai/alinatendler-self-employed/LLM-LHL/runs/r3buk1f8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñá‚ñà‚ñÜ‚ñà‚ñá</td></tr><tr><td>loss</td><td>‚ñà‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.80961</td></tr><tr><td>loss</td><td>0.15433</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-snowball-1</strong> at: <a href='https://wandb.ai/alinatendler-self-employed/LLM-LHL/runs/r3buk1f8' target=\"_blank\">https://wandb.ai/alinatendler-self-employed/LLM-LHL/runs/r3buk1f8</a><br> View project at: <a href='https://wandb.ai/alinatendler-self-employed/LLM-LHL' target=\"_blank\">https://wandb.ai/alinatendler-self-employed/LLM-LHL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250601_162703-r3buk1f8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"alinatendler-self-employed\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"LLM-LHL\",\n",
    "    # Track hyperparameters and run metadata.\n",
    "    config={\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"dataset\": \"CIFAR-100\",\n",
    "        \"epochs\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Simulate training.\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2**-epoch - random.random() / epoch - offset\n",
    "    loss = 2**-epoch + random.random() / epoch + offset\n",
    "\n",
    "    # Log metrics to wandb.\n",
    "    run.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# Finish the run and upload any remaining data.\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
